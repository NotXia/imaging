\documentclass[11pt]{article}
\usepackage{algorithm2e}
\usepackage[italian]{babel}
\usepackage[document]{ragged2e}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage{cancel}
\usepackage{float}
\usepackage{mathtools}
\usepackage[margin=3cm]{geometry}
\usepackage{subfig}
\usepackage{mwe}
\usepackage{hyperref}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\begin{document}
\graphicspath{ {./img/} }
\begin{titlepage}
    \begin{center}
        \vspace*{1.5cm}
            
        \Huge
        \textbf{IMAGING}
            
        \vspace{0.5cm}
        \LARGE
        Relazione
            
        \vspace{1.5cm}
          
        \begin{minipage}[t]{0.47\textwidth}
        \begin{center}
        	{\large{\bf Cheikh Ibrahim $\cdot$ Zaid}}\\
			{\large Matricola: \texttt{0000974909}}
        \end{center}

		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.47\textwidth}\raggedleft
		\begin{center}
        	{\large{\bf Xia $\cdot$ Tian Cheng}}\\
			{\large Matricola: \texttt{0000975129}}
        \end{center}
		\end{minipage}  
            
        \vspace{6cm}
            
        Anno accademico\\
        $2021 - 2022$
            
        \vspace{0.8cm}
            
            
        \Large
        Corso di Calcolo Numerico\\
        Alma Mater Studiorum $\cdot$ Università di Bologna\\
            
    \end{center}
\end{titlepage}
\pagebreak

\section{Introduzione}
Il progetto consiste nel ricostruire un'immagine a partire da una sua istanza alterata da uno sfocamento noto e un rumore casuale.\\
Si tratta di un problema solitamente affrontato elaborando immagini provenienti da un dispositivo di acquisizione che nel suo processo di cattura deve digitalizzare un segnale analogico. 
Per risolvere tali problemi sono note diverse formulazioni. Quelle impiegate per questo progetto sono:
\begin{itemize}
    \setlength\itemsep{0.05cm}
    \item Minimi quadrati
    \item Minimi quadrati con regolarizzazione di Tikhonov
    \item Minimi quadrati con regolarizzazione tramite variazione totale
\end{itemize}
È noto che risolvere il problema di deblur come minimi quadrati in modo diretto è mal condizionato e per questo si introducono tecniche di regolarizzazione.\\
Per misurare la qualità dei risultati verranno impiegate due metriche:
\begin{itemize}
    \setlength\itemsep{0.05cm}
    \item Mean Squared Error (MSE) \textbf{[AGGIUNGERE DESCRIZIONE]}
    \item Peak Signal-to-Noise Ratio (PSNR) \textbf{[AGGIUNGERE DESCRIZIONE]}
\end{itemize}

\section{Esecuzione preliminare}
\label{chap:lambda}
Per avere una visione sul comportamento delle varie formulazioni, è stata eseguita una prima sperimentazione sull'immagine in \autoref{fig:originale1}
\begin{figure}[H]
    \centering
    \includegraphics[width=4cm]{esecuzione/originale.png}
    \caption{Immagine di test}
    \label{fig:originale1}
\end{figure}

\subsection{Prima esecuzione}
La prima esecuzione è stata eseguita con un kernel $5 \times 5$ con $\sigma=0.5$ e rumore gaussiano con deviazione standard $]0, 0.05]$.

\subsubsection{Stima di $\lambda$}
Per valutare il risultato dei vari metodi, è necessario prima determinare il valore $\lambda$ del termine di regolarizzazione degli algoritmi che lo prevedono.
Il seguente grafico mostra il valore del PSNR al variare di $\lambda \in [0.01, 1]$ con passo $0.01$:
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{esecuzione/1/tikhonov_lambda.png}
    \caption{$\lambda=0.19$ e $\texttt{PSNR} \simeq 24.79$}
    \label{fig:tikhonov_lambda1}
\end{figure}
Analogamente, il seguente grafico mostra la variazione del PSNR per $\lambda \in [0.01, 1]$ con passo $0.01$:
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{esecuzione/1/tv_lambda.png}
    \caption{$\lambda=0.07$ e $\texttt{PSNR} \simeq 36.74$}
    \label{fig:tv_lambda1}
\end{figure}

\subsubsection{Esecuzione}
I risultati della prima esecuzione sono osservabili in \autoref{fig:deblur1}.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{esecuzione/1/deblur.png}
    \caption{Risultato sulle varie formulazioni}
    \label{fig:deblur1}
\end{figure}

\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
    & Acquisita & Naive & Tikhonov CG & Tikhonov GD & Variazione totale \\ 
    \hline
    MSE & $0.2699 \cdot 10^{-2}$ & $0.2047 \cdot 10^{0}$ & $0.33197 \cdot 10^{-2}$ & $0.3320 \cdot 10^{-2}$ & $0.2119 \cdot 10^{-3}$ \\ 
    PSNR & $25.6878$ & $6.8874$ & $24.7890$ & $24.7890$ & $36.7382$ \\ 
    Iter. & & 140 & 14 & 48 & 14 \\ 
    \hline
    \end{tabular}
\end{center}

Come atteso, il risultato ottenuto con la formulazione come problema ai minimi quadrati senza regolarizzazione ha prodotto un'immagine molto distante dall'originale.\\
Utilizzando la regolarizzazione di Tikhonov, si è ottenuto un risultato quasi invariato rispetto all'immagine acquisita se non addirittura peggiore; mentre a livello di velocità, il metodo del gradiente ha richiesto più iterazioni rispetto al metodo del gradiente coniugato.\\
Con la regolarizzazione tramite variazione totale, il risultato ottenuto è invece migliore rispetto agli altri metodi e molto vicina all'immagine originale.\\
La ragione per cui Tikhonov ha prodotto tale risultato è probabilmente dovuto al fatto che il rumore "sovrasta" il blur.

\subsection{Seconda esecuzione}
Per vedere le prestazioni di Tikhonov in uno scenario differente, è stata effettuata una seconda esecuzione sulla stessa immagine di partenza con blur ottenuto da un kernel $24 \times 24$ con $\sigma=3$ e rumore con deviazione standard $]0, 0.05]$.

\subsubsection{Stima di $\lambda$}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzione/2/tikhonov_lambda.png}
        \caption{$\lambda=0.09$ e $\texttt{PSNR} \simeq 28.09$}
        \label{fig:tikhonov_lambda2}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzione/2/tv_lambda.png}
        \caption{$\lambda=0.04$ e $\texttt{PSNR} \simeq 35.09$}
        \label{fig:tv_lambda2}
    \end{minipage}
\end{figure}

\subsubsection{Esecuzione}
I risultati sono rappresentati in \autoref{fig:deblur2}.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{esecuzione/2/deblur.png}
    \caption{Risultato sulle varie formulazioni}
    \label{fig:deblur2}
\end{figure}

\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
    & Acquisita & Naive & Tikhonov CG & Tikhonov GD & Variazione totale \\ 
    \hline
    MSE & $0.3204 \cdot 10^{-2}$ & $0.5719 \cdot 10^{-1}$ & $0.1553 \cdot 10^{-2}$ & $0.1553 \cdot 10^{-2}$ & $0.3094 \cdot 10^{-3}$ \\ 
    PSNR & $24.9437$ & $-7.5735$ & $28.0889$ & $28.0890$ & $35.0944$ \\ 
    Iter. & & 200 (max) & 18 & 99 & 29 \\ 
    \hline
    \end{tabular}
\end{center}

Anche in questo caso, il metodo naive non ha prodotto soluzioni accettabili, mentre la regolarizzazione tramite variazione totale, come nel caso precedente, ha prodotto il risultato migliore.\\
Il metodo regolarizzato con Tikhonov invece, a differenza dell'esecuzione precedente, ha prodotto un risultato migliore dell'immagine acquisita e, 
analogamente, il numero di iterazioni del metodo che minimizza con il gradiente coniugato è minore rispetto al metodo del gradiente.

\section{Confronto tra gradiente coniugato e metodo del gradiente}
\label{chap:confronto}
Si analizzano ora le prestazioni di Tikhonov utilizzando i due metodi di discesa implementati. \\

\subsection{Prima esecuzione}
Per una prima sperimentazione si è usata la \autoref{fig:originale1} con kernel $5 \times 5$ con $\sigma=0.5$ e rumore gaussiano con deviazione standard $]0, 0.05]$.\\
I risultati ottenuti sono i seguenti:
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/1/tol_iter.png}
    \caption{Iterazioni al variare della tolleranza}
    \label{fig:tol_iter1}
\end{figure}
È immediato notare che il metodo del gradiente coniugato impiega meno iterazioni rispetto al metodo del gradiente.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/1/tol_psnr.png}
    \caption{PSNR al variare della tolleranza}
    \label{fig:tol_psnr1}
\end{figure}
Il risultato ottenuto mostra che i due metodi, al variare della tolleranza, convergono allo stesso risultato.\\
È però presente un comportamento controintuitivo in vicinanza di valori di tolleranza elevati. 
Infatti si ottiene un PSNR maggiore in corrispondenza di tali valori, in altri termini, si ottiene un'immagine più fedele all'originale con meno iterazioni, mentre il risultato peggiora nella continuazione dell'esecuzione (Tale problematica verrà approfondita nella \autoref{chap:semiconv}).

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/1/funzione_obiettivo.png}
    \caption{Andamento della funzione obiettivo}
    \label{fig:obiettivo1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/1/norma_gradiente.png}
    \caption{Andamento della norma del gradiente}
    \label{fig:gradiente1}
\end{figure}
Come atteso dai risultati precedenti, per il metodo del gradiente coniugato la decrescita della funzione obiettivo è maggiore rispetto al metodo del gradiente. 
Lo stesso risultato è osservabile con la norma del gradente che nel caso del gradiente coniugato esegue "salti" di dimensione maggiore.

\subsection{Seconda esecuzione}
Per una seconda valutazione si è usato un kernel $24 \times 24$ con $\sigma=3$ e rumore gaussiano con deviazione standard $]0, 0.05]$.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/2/tol_iter.png}
    \caption{Iterazioni al variare della tolleranza}
    \label{fig:tol_iter2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/2/tol_psnr.png}
    \caption{PSNR al variare della tolleranza}
    \label{fig:tol_psnr2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/2/funzione_obiettivo.png}
    \caption{Andamento della funzione obiettivo}
    \label{fig:obiettivo2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{iterazioni_cg_gd/2/norma_gradiente.png}
    \caption{Andamento della norma del gradiente}
    \label{fig:gradiente2}
\end{figure}

I risultati ottenuto sono in linea con quelli precedenti ed evidenziano che le prestazioni del gradiente coniugato sono maggiori rispetto a quelle del metodo del gradiente, come atteso dalle valutazioni teoriche.

\section{Semi-convergenza}
\label{chap:semiconv}
Nella \autoref{chap:confronto} è emerso il problema per cui l'immagine ottenuta con meno iterazioni è migliore rispetto a quella ottenuta quando il metodo raggiunge convergenza. 
Tale problema è noto come semi-convergenza, ovvero metodi per il quale il raggiungimento dell'ottimo non corrisponde al soddisfacimento delle condizioni di convergenza 
e per questo le iterazioni successive peggiorano il risultato.\\
Nel contesto del deblur, il problema di semi-convergenza è causato dal rumore aggiunto all'immagine. È noto che il metodo naive è quello che più viene condizionato dal rumore e per questa ragione vengono introdotti i metodi di regolarizzazione.\\
Si analizza quindi il risultato dei vari metodi analizzando l'andamento dell'iterato $x_{k}$:

\subsection{Analisi con rumore}
\subsubsection{Metodo naive}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{semiconvergenza/1/psnr_naive.png}
    \caption{PSNR al variare del numero delle iterazioni}
    \label{fig:semiconv_psnr_naive1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{semiconvergenza/1/deblur_naive.png}
    \caption{Risultato ottenuto}
    \label{fig:semiconv_deblur_naive1}
\end{figure}
Come atteso, il risultato viene distorto molto rapidamente e non viene raggiunta convergenza nel punto ottimo.

\subsubsection{Regolarizzazione di Tikhonov}
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{semiconvergenza/1/psnr_tikhonov.png}
    \caption{PSNR al variare del numero delle iterazioni}
    \label{fig:semiconv_psnr_tikhonov1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{semiconvergenza/1/deblur_tikhonov.png}
    \caption{Risultato ottenuto}
    \label{fig:semiconv_deblur_tikhonov1}
\end{figure}
La regolarizzazione di Tikhonov viene introdotta per ridurre l'impatto del rumore sul risultato finale. 
Infatti si nota che, nonostante sia ancora presente il problema di semi-convergenza, dopo il punto di ottimo l'errore decresce di una quantità più contenuta rispetto al metodo naive fino ad assumere un comportamento asintotico.

\subsubsection{Regolarizzazione tramite variazione totale}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{semiconvergenza/1/psnr_tv.png}
    \caption{PSNR al variare del numero delle iterazioni}
    \label{fig:semiconv_psnr_tv1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{semiconvergenza/1/deblur_tv.png}
    \caption{Risultato ottenuto}
    \label{fig:semiconv_deblur_tv1}
\end{figure}
Regolarizzando tramite variazione totale invece, il problema di semi-convergenza è assente e il raggiungimento dell'ottimo avviene contemporaneamente al soddisfacimento delle condizioni di convergenza.

\subsubsection{Considerazioni finali}
Le prove precedenti sono state eseguite con kernel $5 \times 5$ con $\sigma=0.5$ e kernel $24 \times 24$ con $\sigma=3$, entrambi i casi con rumore con deviazione standard $]0, 0.05]$.\\
Nel caso generale, non è possibile risolvere il problema di semi-convergenza interrompendo l'esecuzione quando si rileva l'aumento dell'errore, 
infatti come si è visto in \autoref{fig:semiconv_psnr_tikhonov1} nel caso del metodo del gradiente, l'andamento del PSNR assume più punti di massimo locale.

\subsection{Analisi senza rumore}
Si eseguono ora gli stessi esperimenti su un'immagine a cui è stato applicato un blur senza aggiungere rumore.
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{semiconvergenza/3/psnr_naive.png}
    \caption{PSNR al variare del numero delle iterazioni con il metodo naive}
    \label{fig:semiconv_psnr_naive3}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{semiconvergenza/3/psnr_tikhonov.png}
    \caption{PSNR al variare del numero delle iterazioni con regolarizzazione di Tikhonov}
    \label{fig:semiconv_deblur_tikhonov3}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{semiconvergenza/3/psnr_tv.png}
    \caption{PSNR al variare del numero delle iterazioni con regolarizzazione tramite variazione totale}
    \label{fig:semiconv_deblur_tv3}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{semiconvergenza/3/deblur_all.png}
    \caption{Risultato ottenuto}
    \label{fig:semiconv_deblur3}
\end{figure}
Come atteso tutti i metodi hanno raggiunto convergenza e ottimo contemporaneamente e il risultato, in assenza di rumore, si presenta molto simile indipendentemente dalla formulazione.

\section{Risultati su esecuzioni multiple}
\subsection{Esecuzioni su un dataset generato casualmente}
Per una valutazione complessiva dei metodi, sono stati eseguiti dei test su un dataset di immagini contenenti poligoni regolari generati casualmente 
a cui è stato applicato un blur con kernel $9 \times 9$ con $\sigma_{blur}=1.3$ e rumore con $sigma_{noise}=0.05$.\\
I risultati sono i seguenti:
\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzioni_multiple/100/psnr1.png}
        \label{fig:100_psnr1}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzioni_multiple/100/psnr2.png}
        \label{fig:100_psnr2}
    \end{minipage}
    \caption{Boxplot al variare del PSNR}
\end{figure}
\begin{center}
    \begin{tabular}{ |c|c|c| }
    \hline
    & Tikhonov & Variazione totale \\ 
    \hline
    Intervallo (\emph{outlier} esclusi) & $[26.03, 28.25]$ & $[35.71, 39.58]$ \\
    Media & $26.39$ & $36.49$ \\
    Mediana & $26.53$ & $36.76$ \\
    Deviazione standard & $1.05$ & $1.38$ \\
    \hline
    \end{tabular}
\end{center}
In linea con i risultati precedenti, il metodo naive ha prodotto soluzioni non accettabili, 
mentre i metodi regolarizzati hanno un comportamento migliore ed è evidente che i risultati ottenuti regolarizzando con variazione totale sono migliori rispetto a quelli ottenuti con Tikhonov.\\

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzioni_multiple/100/iter1.png}
        \label{fig:100_iter1}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=7cm]{esecuzioni_multiple/100/iter2.png}
        \label{fig:100_iter2}
    \end{minipage}
    \caption{Boxplot al variare del numero di iterazioni}
\end{figure}

\begin{center}
    \begin{tabular}{ |c|c|c| }
    \hline
    & Tikhonov & Variazione totale \\ 
    \hline
    Intervallo (\emph{outlier} esclusi) & $[16, 18]$ & $[15, 28]$ \\
    Media & $16.79$ & $19.59$ \\
    Mediana & $17$ & $21$ \\
    Deviazione standard & $0.75$ & $5.47$ \\
    \hline
    \end{tabular}
\end{center}
Anche a livello di numero di iterazioni, il metodo naive ha avuto il risultato peggiore, terminando raggiungendo il numero massimo di iterazioni.\\
La regolarizzazione di Tikhonov ha richiesto un numero di iterazioni contentuto e coerente, mentre la regolarizzazione tramite variazione totale ha mostrato dati più diradati e meno prevedibili.\\
In linea di massima, sperimentalmente si è notato che il tempo di esecuzione richiesto con il metodo tramite variazione totale è stato in generale maggiore rispetto a Tikhonov.

\subsection{Esecuzioni sul dataset}
Rispetto al dataset utilizzato, sono state eseguite diverse misurazioni al variare della dimensione del kernel, del valore di $\sigma$ e della deviazione standard del rumore. 
Per tutte le esecuzioni è stato scelto il valore del termine di regolarizzazione utilizzando lo stesso procedimento nella \autoref{chap:lambda}.\\
I risultati in forma aggregata sono i seguenti:

\begin{center}
    \begin{tabular}{ |cc|c|c|c|c|c|c|c| }
    \hline
    & & & \multicolumn{3}{c|}{MSE} & \multicolumn{3}{c|}{PSNR} \\
    \hline
    \multicolumn{2}{|c|}{Kernel} & Noise & Naive & Tikhonov & Var. tot. & Naive & Tikhonov & Var. tot. \\ 
    \hline
	$7 \times 7$ & $\sigma=0.5$ & $\sigma=0.05$ & $0.19 \cdot 10^{0}$ & $0.40 \cdot 10^{-2}$ & $0.21 \cdot 10^{-3}$ & 7.14 & 24.16 & 36.94 \\
	$7 \times 7$ & $\sigma=1$ & $\sigma=0.05$ & $0.10 \cdot 10^{1}$ & $0.27 \cdot 10^{-2}$ & $0.24 \cdot 10^{-3}$ & -0.16 & 25.85 & 36.42 \\
    $7 \times 7$ & $\sigma=1.3$ & $\sigma=0.05$ & $0.76 \cdot 10^{0}$ & $0.24 \cdot 10^{-2}$ & $0.26 \cdot 10^{-3}$ & 1.21 & 26.36 & 36.07 \\
    $7 \times 7$ & $\sigma=3$ & $\sigma=0.05$ & $0.12 \cdot 10^{1}$ & $0.17 \cdot 10^{-2}$ & $0.34 \cdot 10^{-3}$ & -0.63 & 27.8 & 34.88 \\
	\hline
    $9 \times 9$ & $\sigma=0.5$ & $\sigma=0.05$ & $0.19 \cdot 10^{0}$ & $0.40 \cdot 10^{-2}$ & $0.21 \cdot 10^{-3}$ & 7.13 & 24.16 & 36.9 \\
	$9 \times 9$ & $\sigma=1$ & $\sigma=0.05$ & $0.10 \cdot 10^{1}$ & $0.27 \cdot 10^{-2}$ & $0.23 \cdot 10^{-3}$ & -0.17 & 25.85 & 36.42 \\
	$9 \times 9$ & $\sigma=1.3$ & $\sigma=0.05$ & $0.76 \cdot 10^{0}$ & $0.24 \cdot 10^{-2}$ & $0.25 \cdot 10^{-3}$ & 1.18 & 26.35 & 36.16 \\
    $9 \times 9$ & $\sigma=3$ & $\sigma=0.05$ & $0.53 \cdot 10^{0}$ & $0.17 \cdot 10^{-2}$ & $0.36 \cdot 10^{-3}$ & 2.73 & 27.77 & 34.68 \\
    \hline
    $24 \times 24$ & $\sigma=0.5$ & $\sigma=0.05$ & $0.42 \cdot 10^{0}$ & $0.37 \cdot 10^{-2}$ & $0.19 \cdot 10^{-3}$ & 3.81 & 24.48 & 37.21 \\
    $24 \times 24$ & $\sigma=1$ & $\sigma=0.05$ & $0.81 \cdot 10^{0}$ & $0.28 \cdot 10^{-2}$ & $0.23 \cdot 10^{-3}$ & 0.92 & 25.65 & 36.45 \\
    $24 \times 24$ & $\sigma=1.3$ & $\sigma=0.05$ & $0.78 \cdot 10^{0}$ & $0.25 \cdot 10^{-2}$ & $0.25 \cdot 10^{-3}$ & 1.09 & 26.21 & 36.16 \\
    $24 \times 24$ & $\sigma=3$ & $\sigma=0.05$ & $0.35 \cdot 10^{0}$ & $0.18 \cdot 10^{-2}$ & $0.35 \cdot 10^{-3}$ & 4.58 & 27.64 & 34.71 \\
    \hline
	$9 \times 9$ & $\sigma=1.3$ & $\sigma=0.1$ & $0.31 \cdot 10^{1}$ & $0.51 \cdot 10^{-2}$ & $0.42 \cdot 10^{-3}$ & -4.87 & 23.07 & 33.85 \\
	$24 \times 24$ & $\sigma=3$ & $\sigma=0.1$ & $0.14 \cdot 10^{1}$ & $0.36 \cdot 10^{-2}$ & $0.53 \cdot 10^{-3}$ & -1.51 & 24.63 & 32.88 \\
    \hline
    \end{tabular}
\end{center}


\end{document}

% 1:
% 0.001427304780871977
% 0.0010440853805177704
% 3.6337175788027345e-05

% 2:
% 0.01103051952255651
% 0.00068798017031743
% 6.11848062920853e-05

% 3:
% 0.004990966138935439
% 0.0006347474944564572
% 6.187582107283506e-05

% 4:
% 0.007003851203980547
% 0.0004514264472757439
% 8.970527774433062e-05

% 5:
% 0.345734
% 0.004309635657454127

% 0.0017673775
% 0.0004449716368980724

% 0.00036570062500000005
% 0.0001304443092270965

% 6:
% 3.07889625
% 0.02764732760028541

% 0.005101189999999999
% 0.0014027355360687202

% 0.00041240925
% 7.33073366174048e-05

% 7:
% 1.4168262500000002
% 0.012378831666902164

% 0.0035578874999999998
% 0.0009489450024072786

% 0.00053522175
% 0.00010856108530655679